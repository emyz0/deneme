from sentence_transformers import SentenceTransformer, util

# --- 1. MODEL VE BÄ°LGÄ° TABANI ---

# CÃ¼mleleri sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼ren embedding modeli
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# KÃ¼Ã§Ã¼k bir bilgi tabanÄ± (Ã¶rnek metinler)
knowledge_base = [
    "Ä°stanbul TÃ¼rkiye'nin en kalabalÄ±k ÅŸehridir.",
    "Ay dÃ¼nyadan yaklaÅŸÄ±k 384.000 kilometre uzaktadÄ±r.",
    "Yapay zeka, verilerden Ã¶ÄŸrenebilen bilgisayar sistemlerini tanÄ±mlar.",
    "BoÄŸaziÃ§i KÃ¶prÃ¼sÃ¼ 15 Temmuz Åehitler KÃ¶prÃ¼sÃ¼ olarak da bilinir."
]

# --- 2. KULLANICI SORUSU ---
question = input("Soru gir: ")

# --- 3. EN Ä°LGÄ°LÄ° BÄ°LGÄ°YÄ° BUL ---
question_embedding = embed_model.encode(question, convert_to_tensor=True)
knowledge_embeddings = embed_model.encode(knowledge_base, convert_to_tensor=True)

cosine_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)
best_match_idx = cosine_scores.argmax().item()

retrieved_text = knowledge_base[best_match_idx]

print(f"\nğŸ” En alakalÄ± bilgi: {retrieved_text}")

# --- 4. BASÄ°T CEVAP ÃœRETÄ°CÄ° ---
# Basit bir mantÄ±k: bulunan bilgiyi cÃ¼mleyle birleÅŸtiriyoruz
if "kim" in question.lower():
    answer = f"{retrieved_text} Bu bilgi, sorduÄŸun kiÅŸiyle ilgili olabilir."
elif "ne" in question.lower() or "nedir" in question.lower():
    answer = f"{retrieved_text} Bu, sorunun cevabÄ± olabilir."
else:
    answer = f"Soruna gÃ¶re en alakalÄ± bilgi: {retrieved_text}"

print(f"\nğŸ’¬ Cevap: {answer}")











pip install numpy
pip install scikit-learn
pip install sentence-transformers
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity # Benzerlik hesaplamasÄ± iÃ§in

# --- 1. Dizin OluÅŸturma AÅŸamasÄ± ---

# 1.1. Veri KaynaÄŸÄ± (BasitleÅŸtirilmiÅŸ)
documents = [
    "Retriever-Augmented Generation (RAG) 2020'de tanÄ±tÄ±ldÄ±.",
    "BÃ¼yÃ¼k Dil Modelleri (LLM) eÄŸitim verileri dÄ±ÅŸÄ±ndaki bilgilere eriÅŸmek iÃ§in RAG kullanÄ±r.",
    "VektÃ¶r veritabanÄ±, RAG sistemlerinin Ã¶nemli bir bileÅŸenidir.",
    "Ä°stanbul TÃ¼rkiye'nin en kalabalÄ±k ÅŸehridir.",
    "RAG, halÃ¼sinasyonlarÄ± azaltmaya yardÄ±mcÄ± olabilir."
]

# 1.2. GÃ¶mme Modeli YÃ¼kleme
# GerÃ§ek uygulamada daha gÃ¼Ã§lÃ¼ modeller kullanÄ±lÄ±r (Ã–rn: all-MiniLM-L6-v2)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# 1.3. Verileri GÃ¶mme ve VektÃ¶r VeritabanÄ±na Depolama
doc_embeddings = embedding_model.encode(documents)
print(f"Toplam {len(doc_embeddings)} belge parÃ§asÄ± gÃ¶mÃ¼ldÃ¼.")

# VektÃ¶r veritabanÄ± (Basit NumPy dizisi)
vector_db = {
    "texts": documents,
    "vectors": doc_embeddings
}

# --- 2. Geri Alma ve Ãœretim AÅŸamasÄ± ---

def get_rag_response(query, vector_db, top_k=2):
    # 2.1. Sorgu GÃ¶mme
    query_embedding = embedding_model.encode([query])

    # 2.2. Ä°lgili Bilgiyi Geri Alma (Benzerlik AramasÄ±)
    # KosinÃ¼s BenzerliÄŸi hesaplama
    similarities = cosine_similarity(query_embedding, vector_db["vectors"])[0]

    # En yÃ¼ksek benzerliÄŸe sahip k-adet belgeyi bulma
    top_k_indices = np.argsort(similarities)[::-1][:top_k]
    retrieved_chunks = [vector_db["texts"][i] for i in top_k_indices]
    
    # Geri AlÄ±nan BaÄŸlamÄ± gÃ¶sterme
    print("\n--- Geri AlÄ±nan BaÄŸlam (Context) ---")
    for chunk in retrieved_chunks:
        print(f"- {chunk}")
    print("----------------------------------\n")

    # 2.3. Ä°stem ArtÄ±rma
    context = "\n".join(retrieved_chunks)
    
    # LLM'ye gÃ¶nderilecek nihai istem
    augmented_prompt = f"""AÅŸaÄŸÄ±daki baÄŸlamÄ± kullanarak soruyu yanÄ±tlayÄ±n:
    
    BaÄŸlam: 
    ---
    {context}
    ---
    
    Soru: {query}
    
    YanÄ±tÄ±nÄ±zÄ± yalnÄ±zca verilen baÄŸlamdaki bilgilere dayanarak oluÅŸturun.
    """

    # 2.4. YanÄ±t Ãœretme (GerÃ§ek LLM Ã§aÄŸrÄ±sÄ± yerine basit bir yer tutucu)
    # GerÃ§ek uygulamada burasÄ± OpenAI, Gemini, HuggingFace LLM API Ã§aÄŸrÄ±sÄ± olacaktÄ±r.
    
    if "RAG" in context or "LLM" in context:
        generated_response = f"LLM YanÄ±tÄ±: (BaÄŸlamÄ± kullanarak Ã¼retildi) RAG'Ä±n avantajlarÄ±, halÃ¼sinasyonlarÄ± azaltmasÄ± ve LLM'lere gÃ¼ncel veya Ã¶zel bilgi saÄŸlamasÄ±dÄ±r. Ã–rneÄŸin, '{retrieved_chunks[0]}' cÃ¼mlesi bir Ã¶rnektir."
    else:
        generated_response = "LLM YanÄ±tÄ±: (BaÄŸlamda yeterli bilgi yoktu) ÃœzgÃ¼nÃ¼m, verilen baÄŸlamda bu soruya yanÄ±t verecek yeterli bilgi bulunmamaktadÄ±r."
        
    print(f"--- LLM'ye GÃ¶nderilen ArtÄ±rÄ±lmÄ±ÅŸ Ä°stem (Ä°lk KÄ±sÄ±m) ---\n{augmented_prompt[:250]}...\n----------------------------------")
    return generated_response

# --- Ã–rnek KullanÄ±m ---
user_query = "RAG kullanmanÄ±n faydalarÄ± nelerdir?"
response = get_rag_response(user_query, vector_db)
print("Nihai YanÄ±t:", response)

print("\n--- AlakasÄ±z Sorgu Ã–rneÄŸi ---")
user_query_2 = "TÃ¼rkiye'nin baÅŸkenti neresidir?"
response_2 = get_rag_response(user_query_2, vector_db)
print("Nihai YanÄ±t:", response_2)


